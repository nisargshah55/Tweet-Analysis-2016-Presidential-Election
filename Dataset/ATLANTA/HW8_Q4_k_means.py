__author__ = 'fengchen'

from sklearn.linear_model import LogisticRegression
from sklearn import svm
import pylab as pl
import numpy as np
from sklearn import cross_validation
from sklearn.grid_search import GridSearchCV
from sklearn.cluster import KMeans

from sklearn.metrics.pairwise import cosine_similarity
import json

stopwords = ["Bernie","Hillary","Cruz","Trump"]

tweets = []
for line in open('AtlantaF.txt').readlines():
    tweets.append(json.loads(line))

# Extract the vocabulary of keywords
vocab = dict()
for tweet_id, tweet_text in tweets:
    for term in tweet_text.split():
        term = term.lower()
        if len(term) > 2 and term not in stopwords:
            if vocab.has_key(term):
                vocab[term] = vocab[term] + 1
            else:
                vocab[term] = 1

# Remove terms whose frequencies are less than a threshold (e.g., 15)
vocab = {term: freq for term, freq in vocab.items() if freq > 20}
# Generate an id (starting from 0) for each term in vocab
vocab = {term: idx for idx, (term, freq) in enumerate(vocab.items())}

# Generate X
X = []
for tweet_id, tweet_text in tweets:
    x = [0] * len(vocab)
    terms = [term for term in tweet_text.split() if len(term) > 2]
    for term in terms:
        if vocab.has_key(term):
            x[vocab[term]] += 1
    X.append(x)

# K-means clustering
km = KMeans(n_clusters = 4, n_init = 100) # try 100 different initial centroids
km.fit(X)

cluster = []
cluster_stat = dict()
# Print tweets that belong to cluster 2
for idx, cls in enumerate(km.labels_):
    if cluster_stat.has_key(cls):
        cluster_stat[cls] += 1
    else:
        cluster_stat[cls] = 1
    open('cluster-{0}.txt'.format(cls), 'a').write(json.dumps(tweets[idx]) + '\r\n')

print 'basic information about the clusters that are generated by the k-means clustering algorithm: \r\n'
print 'total number of clusters: {0}\r\n'.format(len(cluster_stat))
for cls, count in cluster_stat.items():
    print 'cluster {0} has {1} tweets'.format(cls, count)
